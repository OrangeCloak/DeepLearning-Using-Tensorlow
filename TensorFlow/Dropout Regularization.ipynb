{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2211735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9d9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDropout Regularization : randomly drop out certain neurons in a specific layer by \\na specified rate. By doing so we are removing the  bias of one neuron towards the other.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Dropout Regularization : randomly drop out certain neurons in a specific layer by \n",
    "a specified rate. By doing so we are removing the  bias of one neuron towards the other.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc66be60",
   "metadata": {},
   "source": [
    "# IMPORTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df20013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.1916</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>0.1613</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>0.0662</td>\n",
       "      <td>0.1108</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.0640</td>\n",
       "      <td>0.0726</td>\n",
       "      <td>0.0901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.1158</td>\n",
       "      <td>0.0922</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.2838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.0712</td>\n",
       "      <td>0.0901</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.1497</td>\n",
       "      <td>0.1284</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1285</td>\n",
       "      <td>0.1684</td>\n",
       "      <td>0.1830</td>\n",
       "      <td>0.2127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "118  0.0363  0.0478  0.0298  0.0210  0.1409  0.1916  0.1349  0.1613  0.1703   \n",
       "195  0.0129  0.0141  0.0309  0.0375  0.0767  0.0787  0.0662  0.1108  0.1777   \n",
       "51   0.0131  0.0068  0.0308  0.0311  0.0085  0.0767  0.0771  0.0640  0.0726   \n",
       "7    0.0519  0.0548  0.0842  0.0319  0.1158  0.0922  0.1027  0.0613  0.1465   \n",
       "148  0.0712  0.0901  0.1276  0.1497  0.1284  0.1165  0.1285  0.1684  0.1830   \n",
       "190  0.0156  0.0210  0.0282  0.0596  0.0462  0.0779  0.1365  0.0780  0.1038   \n",
       "\n",
       "         9   ...      51      52      53      54      55      56      57  \\\n",
       "118  0.1444  ...  0.0115  0.0190  0.0055  0.0096  0.0050  0.0066  0.0114   \n",
       "195  0.2245  ...  0.0124  0.0093  0.0072  0.0019  0.0027  0.0054  0.0017   \n",
       "51   0.0901  ...  0.0062  0.0028  0.0040  0.0075  0.0039  0.0053  0.0013   \n",
       "7    0.2838  ...  0.0081  0.0120  0.0045  0.0121  0.0097  0.0085  0.0047   \n",
       "148  0.2127  ...  0.0154  0.0156  0.0054  0.0030  0.0048  0.0087  0.0101   \n",
       "190  0.1567  ...  0.0150  0.0060  0.0082  0.0091  0.0038  0.0056  0.0056   \n",
       "\n",
       "         58      59  60  \n",
       "118  0.0073  0.0033   M  \n",
       "195  0.0024  0.0029   M  \n",
       "51   0.0052  0.0023   R  \n",
       "7    0.0048  0.0053   R  \n",
       "148  0.0095  0.0068   M  \n",
       "190  0.0048  0.0024   M  \n",
       "\n",
       "[6 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://raw.githubusercontent.com/codebasics/deep-learning-keras-tf-tutorial/master/13_dropout_layer/sonar_dataset.csv'\n",
    "res = requests.get(url, allow_redirects=True)\n",
    "with open('sonar_dataset.csv','wb') as file:\n",
    "    file.write(res.content)\n",
    "df = pd.read_csv('sonar_dataset.csv', header=None)\n",
    "\n",
    "df.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adce054",
   "metadata": {},
   "source": [
    "# Data Manuplation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2043d480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f24f593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "            51, 52, 53, 54, 55, 56, 57, 58, 59, 60],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d3406a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "     ..\n",
       "56    0\n",
       "57    0\n",
       "58    0\n",
       "59    0\n",
       "60    0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8cc9bf",
   "metadata": {},
   "source": [
    "# Training ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da8efdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    R\n",
       "1    R\n",
       "2    R\n",
       "3    R\n",
       "4    R\n",
       "Name: 60, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting data into input and output\n",
    "x = df.drop(60,axis='columns')\n",
    "y = df[60]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08b7ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting Y into 1/0 format:\n",
    "y2 = df[60]\n",
    "replaced_words = {\n",
    "    'R' : 1,\n",
    "    'M' : 0\n",
    "}\n",
    "y2 = y2.map(replaced_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dd147f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "203    0\n",
       "204    0\n",
       "205    0\n",
       "206    0\n",
       "207    0\n",
       "Name: 60, Length: 208, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51953e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing Data into train test split:\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_test , y_train , y_test = train_test_split(x , y2 , test_size = 0.25 , random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6973eda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.1023</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0792</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0687</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0865</td>\n",
       "      <td>0.1182</td>\n",
       "      <td>0.0999</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>0.2318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.0231</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.1351</td>\n",
       "      <td>0.1063</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.0778</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.1711</td>\n",
       "      <td>0.1483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "23   0.0115  0.0150  0.0136  0.0076  0.0211  0.1058  0.1023  0.0440  0.0931   \n",
       "50   0.0353  0.0713  0.0326  0.0272  0.0370  0.0792  0.1083  0.0687  0.0298   \n",
       "163  0.0072  0.0027  0.0089  0.0061  0.0420  0.0865  0.1182  0.0999  0.1976   \n",
       "78   0.0231  0.0351  0.0030  0.0304  0.0339  0.0860  0.1738  0.1351  0.1063   \n",
       "60   0.0130  0.0006  0.0088  0.0456  0.0525  0.0778  0.0931  0.0941  0.1711   \n",
       "\n",
       "         9   ...      50      51      52      53      54      55      56  \\\n",
       "23   0.0734  ...  0.0107  0.0091  0.0016  0.0084  0.0064  0.0026  0.0029   \n",
       "50   0.0880  ...  0.0098  0.0163  0.0242  0.0043  0.0202  0.0108  0.0037   \n",
       "163  0.2318  ...  0.0092  0.0078  0.0071  0.0081  0.0034  0.0064  0.0037   \n",
       "78   0.0347  ...  0.0154  0.0106  0.0097  0.0022  0.0052  0.0072  0.0056   \n",
       "60   0.1483  ...  0.0092  0.0078  0.0041  0.0013  0.0011  0.0045  0.0039   \n",
       "\n",
       "         57      58      59  \n",
       "23   0.0037  0.0070  0.0041  \n",
       "50   0.0096  0.0093  0.0053  \n",
       "163  0.0036  0.0012  0.0037  \n",
       "78   0.0038  0.0043  0.0030  \n",
       "60   0.0022  0.0023  0.0016  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec1c6b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building ANN:\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40591d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 60)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c514fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(60 , input_dim = 60 , activation='relu'),\n",
    "    Dense(30 , activation='relu'),\n",
    "    Dense(15 , activation='relu'),\n",
    "    Dense(1 , activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7586674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4205d5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2314 - accuracy: 0.9038\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2249 - accuracy: 0.9167\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1915 - accuracy: 0.9167\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1403 - accuracy: 0.9679\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.9551\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1386 - accuracy: 0.9615\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1063 - accuracy: 0.9679\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0973 - accuracy: 0.9808\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0944 - accuracy: 0.9744\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0993 - accuracy: 0.9744\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0749 - accuracy: 0.9872\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0671 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0632 - accuracy: 0.9936\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0506 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0462 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0517 - accuracy: 0.9936\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0336 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0328 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b2f92e7850>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train , y_train , epochs=50, batch_size = 8)\n",
    "#Batch size is for mini gradient descent\n",
    "#Accuracy 1 means model has overfit the data!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d977e395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4528 - accuracy: 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4527840316295624, 0.8653846383094788]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39be793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "[2.0615418e-05 9.9881959e-01 1.7571788e-02 4.8899200e-02 9.9650908e-01\n",
      " 6.3615153e-04 7.4721390e-01 9.9999863e-01 8.4642984e-04 9.9998271e-01]\n",
      "[0. 1. 0. 0. 1. 0. 1. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test).reshape(-1)\n",
    "print(y_pred[:10])\n",
    "\n",
    "#Rounding off to nearest int i.e, 0 or 1 :\n",
    "y_pred2 = np.round(y_pred)\n",
    "print(y_pred2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f57f2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we Try to add dropout layer:\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "651b6c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 1s 2ms/step - loss: 0.7099 - accuracy: 0.4872\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6986 - accuracy: 0.5449\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6761 - accuracy: 0.5769\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6684 - accuracy: 0.5962\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6694 - accuracy: 0.5769\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6561 - accuracy: 0.6218\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6476 - accuracy: 0.6282\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6514 - accuracy: 0.6282\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6476 - accuracy: 0.6538\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6300 - accuracy: 0.6923\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.7051\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6097 - accuracy: 0.6795\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6052 - accuracy: 0.6923\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7372\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5888 - accuracy: 0.6923\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5655 - accuracy: 0.7244\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5565 - accuracy: 0.7308\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5575 - accuracy: 0.7308\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5477 - accuracy: 0.7179\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5370 - accuracy: 0.7372\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5163 - accuracy: 0.7692\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5437 - accuracy: 0.7179\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5277 - accuracy: 0.7372\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4964 - accuracy: 0.7692\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5185 - accuracy: 0.7372\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4938 - accuracy: 0.7628\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4692 - accuracy: 0.7821\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.7756\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4460 - accuracy: 0.7885\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5139 - accuracy: 0.7372\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4707 - accuracy: 0.8077\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.7756\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.7885\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4456 - accuracy: 0.7821\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.8013\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.7821\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.8013\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4249 - accuracy: 0.7949\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4290 - accuracy: 0.8077\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4118 - accuracy: 0.8141\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3838 - accuracy: 0.8333\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.8333\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3974 - accuracy: 0.8397\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3755 - accuracy: 0.8397\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3710 - accuracy: 0.8590\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3987 - accuracy: 0.8397\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.8718\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8590\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3218 - accuracy: 0.8782\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3833 - accuracy: 0.8397\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3611 - accuracy: 0.8590\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3654 - accuracy: 0.8269\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3538 - accuracy: 0.8718\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3550 - accuracy: 0.8077\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8782\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.8462\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3310 - accuracy: 0.8718\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3287 - accuracy: 0.8590\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3161 - accuracy: 0.8654\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8846\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2839 - accuracy: 0.8974\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2628 - accuracy: 0.9103\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.8910\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2664 - accuracy: 0.9038\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.9038\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2746 - accuracy: 0.8846\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2555 - accuracy: 0.8974\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2734 - accuracy: 0.8846\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2594 - accuracy: 0.8910\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.9295\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.9167\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.2491 - accuracy: 0.8782\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2263 - accuracy: 0.9103\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8910\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2204 - accuracy: 0.9103\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2356 - accuracy: 0.9167\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2526 - accuracy: 0.8974\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1774 - accuracy: 0.9487\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2083 - accuracy: 0.9359\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2039 - accuracy: 0.9231\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2345 - accuracy: 0.9038\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1670 - accuracy: 0.9295\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1962 - accuracy: 0.9231\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1993 - accuracy: 0.9231\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1804 - accuracy: 0.9295\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1721 - accuracy: 0.9359\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1531 - accuracy: 0.9487\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1531 - accuracy: 0.9551\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1679 - accuracy: 0.9295\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1988 - accuracy: 0.8910\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1562 - accuracy: 0.9359\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1575 - accuracy: 0.9487\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1976 - accuracy: 0.9231\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1733 - accuracy: 0.9231\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1406 - accuracy: 0.9487\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1645 - accuracy: 0.9423\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9295\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1338 - accuracy: 0.9551\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9359\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1514 - accuracy: 0.9551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b2ffbe9f30>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Sequential([\n",
    "    Dense(60 , input_dim = 60 , activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(30 , activation='relu'),\n",
    "    Dropout(0.15),\n",
    "    Dense(15 , activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1 , activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model1.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model1.fit(x_train,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "140b7be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x000001B2FE873760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.3196 - accuracy: 0.8462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31960099935531616, 0.8461538553237915]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a72a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
