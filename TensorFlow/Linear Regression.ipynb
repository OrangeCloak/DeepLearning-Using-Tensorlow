{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5beef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e7473",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a30e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "dftest = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\")\n",
    "\n",
    "ytrain = dftrain.pop('survived')\n",
    "ytest = dftest.pop('survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c0656",
   "metadata": {},
   "source": [
    "# Encoding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1060ab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VocabularyListCategoricalColumn(key='sex', vocabulary_list=('male', 'female'), dtype=tf.string, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='n_siblings_spouses', vocabulary_list=(1, 0, 3, 4, 2, 5, 8), dtype=tf.int64, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='parch', vocabulary_list=(0, 1, 2, 5, 3, 4), dtype=tf.int64, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='class', vocabulary_list=('Third', 'First', 'Second'), dtype=tf.string, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='deck', vocabulary_list=('unknown', 'C', 'G', 'A', 'B', 'D', 'F', 'E'), dtype=tf.string, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='embark_town', vocabulary_list=('Southampton', 'Cherbourg', 'Queenstown', 'unknown'), dtype=tf.string, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='alone', vocabulary_list=('n', 'y'), dtype=tf.string, default_value=-1, num_oov_buckets=0), NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='fare', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n"
     ]
    }
   ],
   "source": [
    "CATEGORICAL_COLUMNS = ['sex' , 'n_siblings_spouses' , 'parch' , 'class', 'deck' , 'embark_town' , 'alone']\n",
    "NUMERICAL_COLUMNS = ['age' , 'fare']\n",
    "\n",
    "feature_columns = []\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "    vocabulary = dftrain[feature_name].unique()\n",
    "    feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name,vocabulary))\n",
    "    \n",
    "for feature_name in NUMERICAL_COLUMNS:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name , dtype=tf.float32))\n",
    "    \n",
    "print(feature_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2209d",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c018dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Will train data into small batches of 32 rather than passing the whole data at once.\n",
    "# Epochs means how many times the model is going to see the same data. that is feed same data in different order every single time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623eb6b",
   "metadata": {},
   "source": [
    "We are going to use input function to determine epochs and batch size. TensorFlow Model allows us to pass data in form of\n",
    "*/tf.data.Dataset*/  object so we need to convert our pandas data into Dataset using input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b00a36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(data_df , label_df , epochs=10 , shuffle = True , batch_size=32):\n",
    "    \n",
    "    def input_function(): #Inner funciton. this will be returned.\n",
    "        ds = tf.data.Dataset.from_tensor_slices(dict((data_df) , label_df)) # Create as tf.data.Dataset object with data and itd labels\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(1000) #shuffle the dataset . i,e randomize order of data\n",
    "        ds = ds.batch(batch_size).repeat(epochs)\n",
    "        return ds\n",
    "    return input_function\n",
    "\n",
    "train_input_fn = make_input_fn(dftrain , ytrain)\n",
    "eval_input_fn = make_input_fn(dftest , ytest , epochs=1 , shuffle=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c03506",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bed098e3",
   "metadata": {},
   "source": [
    "linear_est = tf.estimator.LinearClassifier(feature_columns = feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1f7b7",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20de79c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dict expected at most 1 argument, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlinear_est\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m result \u001b[38;5;241m=\u001b[39m linear_est\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m      4\u001b[0m clear_output()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py:360\u001b[0m, in \u001b[0;36mEstimator.train\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    357\u001b[0m hooks\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_train_steps_to_hooks(steps, max_steps))\n\u001b[0;32m    359\u001b[0m saving_listeners \u001b[38;5;241m=\u001b[39m _check_listeners_type(saving_listeners)\n\u001b[1;32m--> 360\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaving_listeners\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss for final step: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, loss)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py:1186\u001b[0m, in \u001b[0;36mEstimator._train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1184\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_model_distributed(input_fn, hooks, saving_listeners)\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1186\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_model_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaving_listeners\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py:1212\u001b[0m, in \u001b[0;36mEstimator._train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1209\u001b[0m   training_util\u001b[38;5;241m.\u001b[39m_get_or_create_global_step_read(g)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m features, labels, input_hooks \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_features_and_labels_from_input_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1213\u001b[0m worker_hooks\u001b[38;5;241m.\u001b[39mextend(input_hooks)\n\u001b[0;32m   1214\u001b[0m estimator_spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_model_fn(features, labels, ModeKeys\u001b[38;5;241m.\u001b[39mTRAIN,\n\u001b[0;32m   1215\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py:1048\u001b[0m, in \u001b[0;36mEstimator._get_features_and_labels_from_input_fn\u001b[1;34m(self, input_fn, mode)\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_features_and_labels_from_input_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_fn, mode):\n\u001b[0;32m   1046\u001b[0m   \u001b[38;5;124;03m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m estimator_util\u001b[38;5;241m.\u001b[39mparse_input_fn_result(\n\u001b[1;32m-> 1048\u001b[0m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_input_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py:1141\u001b[0m, in \u001b[0;36mEstimator._call_input_fn\u001b[1;34m(self, input_fn, mode, input_context)\u001b[0m\n\u001b[0;32m   1139\u001b[0m   kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_context\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m input_context\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/cpu:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m-> 1141\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m input_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn [15], line 4\u001b[0m, in \u001b[0;36mmake_input_fn.<locals>.input_function\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput_function\u001b[39m(): \u001b[38;5;66;03m#Inner funciton. this will be returned.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_df\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# Create as tf.data.Dataset object with data and itd labels\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m      6\u001b[0m         ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1000\u001b[39m) \u001b[38;5;66;03m#shuffle the dataset . i,e randomize order of data\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: dict expected at most 1 argument, got 2"
     ]
    }
   ],
   "source": [
    "linear_est.train(train_input_fn)\n",
    "result = linear_est.evaluate()\n",
    "\n",
    "clear_output()\n",
    "print(result['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318fd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
